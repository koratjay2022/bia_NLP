{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The premise of symbolic NLP is well-summarized by John Searle's Chinese . room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\"\n",
    "# text = \"The premise of symbolic NLP is well-summarized by John Searle's Chinese . room experiment: Given a collection of rules e.g., a Chinese phrasebook, with questions and matching answers, the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the premise of symbolic nlp is well-summarized by john searle's chinese . room experiment: given a collection of rules (e.g., a chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other nlp tasks) by applying those rules to the data it confronts.\n"
     ]
    }
   ],
   "source": [
    "lowercased_text = text.lower()\n",
    "print(lowercased_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenize & sexpr Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sexpr_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sexpr_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'premise',\n",
       " 'of',\n",
       " 'symbolic',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'well-summarized',\n",
       " 'by',\n",
       " 'John',\n",
       " \"Searle's\",\n",
       " 'Chinese',\n",
       " '.',\n",
       " 'room',\n",
       " 'experiment:',\n",
       " 'Given',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'rules',\n",
       " '(e.g., a Chinese phrasebook, with questions and matching answers)',\n",
       " ',',\n",
       " 'the',\n",
       " 'computer',\n",
       " 'emulates',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " '(or other NLP tasks)',\n",
       " ' by applying those rules to the data it confronts.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'premise',\n",
       " 'of',\n",
       " 'symbolic',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'well-summarized',\n",
       " 'by',\n",
       " 'John',\n",
       " 'Searle',\n",
       " \"'s\",\n",
       " 'Chinese',\n",
       " '.',\n",
       " 'room',\n",
       " 'experiment',\n",
       " ':',\n",
       " 'Given',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'rules',\n",
       " '(',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'a',\n",
       " 'Chinese',\n",
       " 'phrasebook',\n",
       " ',',\n",
       " 'with',\n",
       " 'questions',\n",
       " 'and',\n",
       " 'matching',\n",
       " 'answers',\n",
       " ')',\n",
       " ',',\n",
       " 'the',\n",
       " 'computer',\n",
       " 'emulates',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " '(',\n",
       " 'or',\n",
       " 'other',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " ')',\n",
       " 'by',\n",
       " 'applying',\n",
       " 'those',\n",
       " 'rules',\n",
       " 'to',\n",
       " 'the',\n",
       " 'data',\n",
       " 'it',\n",
       " 'confronts',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop World\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercased\n",
    "lowercased_text = text.lower()\n",
    "# Tokenize\n",
    "new_text = word_tokenize(lowercased_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " 'i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "stop_world = list(punctuation) + stop\n",
    "stop_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise\n",
      "symbolic\n",
      "nlp\n",
      "well-summarized\n",
      "john\n",
      "searle\n",
      "'s\n",
      "chinese\n",
      "room\n",
      "experiment\n",
      "given\n",
      "collection\n",
      "rules\n",
      "e.g.\n",
      "chinese\n",
      "phrasebook\n",
      "questions\n",
      "matching\n",
      "answers\n",
      "computer\n",
      "emulates\n",
      "natural\n",
      "language\n",
      "understanding\n",
      "nlp\n",
      "tasks\n",
      "applying\n",
      "rules\n",
      "data\n",
      "confronts\n"
     ]
    }
   ],
   "source": [
    "for i in new_text :\n",
    "    if i not in stop_world:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming & Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer,RegexpStemmer,LancasterStemmer,SnowballStemmer\n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "l = LancasterStemmer()\n",
    "r = RegexpStemmer('ing')\n",
    "p = PorterStemmer()\n",
    "s = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LancasterStemmer :-  changing study\n",
      "RegexpStemmer :-  chang study\n",
      "PorterStemmer :-  changing studi\n",
      "SnowballStemmer :-  changing studi\n"
     ]
    }
   ],
   "source": [
    "chageing_stemmer = \"changing studying\"\n",
    "print(\"LancasterStemmer :- \",l.stem(chageing_stemmer))\n",
    "print(\"RegexpStemmer :- \",r.stem(chageing_stemmer))\n",
    "print(\"PorterStemmer :- \",p.stem(chageing_stemmer))\n",
    "print(\"SnowballStemmer :- \",s.stem(chageing_stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chaning'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lemmatizer = \"chaning\"\n",
    "wl.lemmatize(text_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder,TrigramCollocationFinder,ngrams\n",
    "# BigramCollocationFinder = 2 \n",
    "# TrigramCollocationFinder = 3 \n",
    "# ngrams = number of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am jay korat i am 99 yers i am young i am jay i whant to eat pizza'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"i am jay korat i am 99 yers i am young i am jay i whant to eat pizza\"\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = word_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = BigramCollocationFinder.from_words(w)\n",
    "t = TrigramCollocationFinder.from_words(w)\n",
    "n = ngrams(w,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('i', 'am'): 4, ('am', 'jay'): 2, ('jay', 'korat'): 1, ('korat', 'i'): 1, ('am', '99'): 1, ('99', 'yers'): 1, ('yers', 'i'): 1, ('am', 'young'): 1, ('young', 'i'): 1, ('jay', 'i'): 1, ...})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.ngram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('i', 'am', 'jay'): 2, ('am', 'jay', 'korat'): 1, ('jay', 'korat', 'i'): 1, ('korat', 'i', 'am'): 1, ('i', 'am', '99'): 1, ('am', '99', 'yers'): 1, ('99', 'yers', 'i'): 1, ('yers', 'i', 'am'): 1, ('i', 'am', 'young'): 1, ('am', 'young', 'i'): 1, ...})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ngram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'am')\n",
      "('am', 'jay')\n",
      "('jay', 'korat')\n",
      "('korat', 'i')\n",
      "('i', 'am')\n",
      "('am', '99')\n",
      "('99', 'yers')\n",
      "('yers', 'i')\n",
      "('i', 'am')\n",
      "('am', 'young')\n",
      "('young', 'i')\n",
      "('i', 'am')\n",
      "('am', 'jay')\n",
      "('jay', 'i')\n",
      "('i', 'whant')\n",
      "('whant', 'to')\n",
      "('to', 'eat')\n",
      "('eat', 'pizza')\n"
     ]
    }
   ],
   "source": [
    "for i in n:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['my name is xyz sarname','my name is a jay am a good boy']\n",
    "df = pd.DataFrame({'name':l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my name is xyz sarname</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>my name is a jay am a good boy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name\n",
       "0          my name is xyz sarname\n",
       "1  my name is a jay am a good boy"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = cv.fit_transform(df['name']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 5,\n",
       " 'name': 6,\n",
       " 'is': 3,\n",
       " 'xyz': 8,\n",
       " 'sarname': 7,\n",
       " 'jay': 4,\n",
       " 'am': 0,\n",
       " 'good': 2,\n",
       " 'boy': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word sense disambiguation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"A computer mouse (plural mice, also mouses)[nb 1] is a hand-held pointing device that detects two-dimensional motion relative to a surface. This motion is typically translated into the motion of the pointer (called a cursor) on a display, which allows a smooth control of the graphical user interface of a computer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = lesk(word_tokenize(x),\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a particular branch of scientific knowledge'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
